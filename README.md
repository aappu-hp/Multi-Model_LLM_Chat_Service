
# ğŸš€ Multi-Model LLM Chat Service

This project is a **Python-based FastAPI service** that integrates multiple open-source LLMs (e.g., `gemma3`, `tinyllama`) using the `langchain_ollama` library. The service accepts user prompts via HTTP, routes them to the selected model, and returns the model's response along with latency and token count. Logs are persisted in a JSON file for analysis.

---

## âœ¨ Features

1. âœ… **Multi-Model Support**: Route prompts to different models (e.g., `gemma3`, `tinyllama`) using the `model` parameter in the request body.
2. â± **Latency and Token Logging**: Logs round-trip latency and token count for each request/response in a `logs.json` file.
3. ğŸŒ **Simple HTTP API**: Exposes a `/chat` endpoint to accept user prompts and return model responses.
4. ğŸ“ **JSON Logging**: Logs are stored in a structured JSON format for easy analysis.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .gitignore          # Specifies intentionally untracked files to ignore
â”œâ”€â”€ .python-version     # Python version specification (if using pyenv)
â”œâ”€â”€ README.md           # Project documentation (this file)
â”œâ”€â”€ logs.json           # Persistent interaction logs (created after first run)
â”œâ”€â”€ main.py             # FastAPI application entry point
â”œâ”€â”€ pyproject.toml      # Python project metadata and build configuration
â”œâ”€â”€ requirements.txt    # Python dependencies list
â”œâ”€â”€ uv.lock             # UV package manager lock file
```
---

## ğŸ“¦ Prerequisites

- ğŸ **Python**: Version 3.10 or higher  
- ğŸ“¦ **UV Package Manager**: For creating a virtual environment and managing dependencies  
- ğŸ§  **Ollama**: A local LLM server required for running the models  
- ğŸ“¥ **Models**: Ensure `gemma3`, `tinyllama` are pulled using the Ollama CLI

---

## âš™ï¸ Setup Instructions

### 1. ğŸ§¬ Clone the Repository
```bash
git clone https://github.com/aappu-hp/Multi-Model_LLM_Chat_Service.git
cd Multi-Model_LLM_Chat_Service
```

### 2. ğŸ§ª Create a Virtual Environment
Use the `uv` package manager:
```bash
uv venv 
```

Activate the virtual environment:
- **Windows**:
  ```bash
  .venv\Scripts\activate
  ```
- **Linux/Mac**:
  ```bash
  source .venv/bin/activate
  ```

### 3. ğŸ“¦ Install Dependencies
```bash
uv add -r requirements.txt
```

### 4. ğŸ”§ Install Ollama
Download and install the Ollama CLI from [Ollama's official website](https://ollama.com/). Follow the installation instructions for your operating system.

### 5. ğŸ“¥ Pull Required Models
```bash
ollama pull gemma3:1b
ollama pull tinyllama
```

Verify:
```bash
ollama list
```

### 6. â–¶ï¸ Start the Ollama Server
```bash
ollama serve
```

### 7. ğŸš€ Run the FastAPI Application
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

---

## ğŸ“¡ API Usage

### ğŸ”— Endpoint: `/chat`

#### ğŸ“¤ Request
- **Method**: `POST`
- **Content-Type**: `application/json`
- **Body**:
```json
{
  "prompt": "Your input prompt here",
  "model": "gemma3"
}
```

#### ğŸ“¥ Response
- **Status Code**: `200 OK`
```json
{
  "model": "gemma3",
  "response": "Model's response here",
  "latency_ms": 123.45,
  "token_count": 42
}
```

#### âŒ Error Handling
- **Unsupported Model**:
```json
{
  "detail": "Unsupported model 'invalid_model'"
}
```

---

## ğŸ§ª Testing

### ğŸ§ª Using Thunder Client Extension
1. Install the [Thunder Client](https://marketplace.visualstudio.com/items?itemName=rangav.vscode-thunder-client)
2. Create a new `POST` request:
   - **URL**: `http://localhost:8000/chat`
   - **Headers**: `Content-Type: application/json`
   - **Body**:
     ```json
     {
       "prompt": "Hello, world!",
       "model": "gemma3"
     }
     ```
3. Send and verify the response.

### ğŸŒ Using FastAPI Swagger UI
- Visit: [http://localhost:8000/docs](http://localhost:8000/docs)
- Use the interactive Swagger UI to test the `/chat` endpoint

### ğŸ§µ Using cURL
```bash
curl -X POST http://localhost:8000/chat ^
-H "Content-Type: application/json" ^
-d "{\"prompt\": \"Hello, world!\", \"model\": \"gemma3\"}"
```

---

## ğŸ—ƒ Logging

Logs are stored in `logs.json` as:
```json
[
  {
    "timestamp": "2025-07-14T12:34:56",
    "model": "gemma3",
    "prompt": "Your input prompt here",
    "response": "Model's response here",
    "latency_ms": 123.45,
    "token_count": 42
  }
]
```

---

## ğŸ› ï¸ Troubleshooting ğŸ”

| Issue                  | Solution                                               |
|------------------------|--------------------------------------------------------|
| âŒ Model not responding | âœ… Verify `ollama serve` is running                    |
| ğŸš« Import errors        | ğŸ”„ Recreate virtual environment with `uv venv`         |
| âš ï¸ Unsupported model    | ğŸ” Check `ollama list` for available models            |
| ğŸ”Œ Connection refused   | ğŸ–¥ï¸ Ensure API is running on port `8000`               |